<img width="1536" height="1024" alt="image" src="https://github.com/user-attachments/assets/3ddc0bc0-04eb-4916-8a2b-612e21768288" />

# ğŸ“˜ AWS POC â€“ S3 Inventory â†’ AWS Batch â†’ Parquet Pipeline (Terraform)

## ğŸ“Œ Project Overview

This project demonstrates a **real-world AWS data engineering pipeline** built using **Terraform (Infrastructure as Code)**.

The pipeline processes **Amazon S3 Inventory metadata** using **AWS Batch (Fargate)** and converts it into **Parquet files** that are ready for analytics tools like **Databricks** or **Amazon Athena**.

## ğŸ¯ What This POC Solves

* Large S3 buckets can contain **millions of objects**
* Reading them directly is slow and expensive
* S3 Inventory provides metadata efficiently
* AWS Batch enables **parallel processing at scale**
* Parquet format enables **fast analytics**

---

## ğŸ§± High-Level Architecture

```
Source S3 Bucket
â”‚
â”œâ”€â”€ Daily S3 Inventory (CSV metadata)
â”‚
â”œâ”€â”€ AWS Batch â€“ Inventory Splitter (Single Job)
â”‚       â””â”€â”€ inventory-chunks/*.csv
â”‚
â”œâ”€â”€ AWS Batch â€“ Chunk Processor (Array Job)
â”‚       â””â”€â”€ processed/*.parquet
â”‚
â””â”€â”€ Analytics Layer (Databricks / Athena)
```

---

## ğŸ›  AWS Services Used

* Amazon S3
* S3 Inventory
* AWS Batch (Fargate)
* Amazon ECR
* AWS IAM
* Docker
* Terraform

---

## ğŸ“‚ Repository Structure

```
AWS-POC-USING-TERRAFORM-CODE/
â””â”€â”€ AWS-poc/
â””â”€â”€ AWS-poc-step1-clean/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ inventory_splitter.py
â”‚   â”œâ”€â”€ chunk_processor.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ batch.tf
â”œâ”€â”€ ecr_step3.tf
â”œâ”€â”€ iam.tf
â”œâ”€â”€ job_definitions.tf
â”œâ”€â”€ providers.tf
â”œâ”€â”€ s3_inventory.tf
â”œâ”€â”€ source.tf
â”œâ”€â”€ variables.tf
â”œâ”€â”€ terraform.tfvars
â””â”€â”€ README.md
```

âš ï¸ **Do NOT commit**

* `.terraform/`
* `terraform.tfstate*`

---

# ğŸ§© Step-by-Step Pipeline Explanation

---

## âœ… STEP 1: Create Source & Destination S3 Buckets

### ğŸ”¹ What we do

* Use Terraform to:

  * Create a **new source bucket**
  * Create a **new destination bucket**

### ğŸ”¹ Why this step is needed

* Source bucket contains **actual data objects**
* Destination bucket stores:

  * Inventory metadata
  * Chunk files
  * Final Parquet output

### ğŸ”¹ Terraform files involved

* `source.tf`
* `variables.tf`
* `terraform.tfvars`

### ğŸ”¹ What happens internally

* Terraform checks AWS state
* Creates destination bucket if it doesnâ€™t exist
* No data is copied at this stage

### ğŸ”¹ Output

* Two S3 buckets available

ğŸ“¸ **Screenshot to add here**



* AWS S3 Console â†’ Bucket list showing both buckets

---

## âœ… STEP 2: Enable S3 Inventory on Source Bucket

### ğŸ”¹ What we do

* Enable **daily S3 Inventory**
* Inventory is delivered as **CSV files**
* Output bucket = destination bucket

### ğŸ”¹ Why this step is needed

* Listing millions of objects manually is expensive
* S3 Inventory provides:

  * Scalable
  * Cost-efficient
  * Consistent metadata snapshots

### ğŸ”¹ Terraform file involved

* `s3_inventory.tf`
<img width="1546" height="338" alt="image" src="https://github.com/user-attachments/assets/4ef03151-d97a-432b-ae9d-8b8697108be6" />

### ğŸ”¹ What happens internally

1. AWS generates inventory once per day
2. Metadata is written as compressed CSV
3. Files are automatically stored in S3

### ğŸ”¹ What data is included

* Object key (path)
* Size
* Last modified timestamp
* Storage class

ğŸš« **Not included**

* File content
* Parquet data itself

### ğŸ”¹ Output

```
s3://<destination-bucket>/inventory/
â””â”€â”€ YYYY-MM-DD/
    â””â”€â”€ inventory.csv.gz
```

ğŸ“¸ **Screenshot to add here**

* S3 â†’ Inventory configuration
* Inventory CSV file in destination bucket
<img width="1902" height="803" alt="image" src="https://github.com/user-attachments/assets/fa3ef4bd-a70f-4344-ae39-7ab882bec384" />

---

## âœ… STEP 3: Inventory Splitter (AWS Batch â€“ Single Job)

### ğŸ”¹ What we do

* Run **one AWS Batch job**
* Download inventory CSV
* Split it into smaller chunk files

### ğŸ”¹ Why this step is needed

* Inventory CSV can be **very large**
* Parallel processing requires smaller inputs
* Each chunk = independent processing unit

### ğŸ”¹ Code used

* `docker/inventory_splitter.py`

### ğŸ”¹ Terraform involved

* `batch.tf`
* `job_definitions.tf`
* `iam.tf`

### ğŸ”¹ What happens internally

1. Batch pulls Docker image from ECR
2. Script downloads inventory CSV
3. Reads records in memory
4. Splits data into N chunks
5. Uploads chunk files to S3

### ğŸ”¹ Output

```
s3://<destination-bucket>/inventory-chunks/
â””â”€â”€ run_date=YYYY-MM-DD/
    â”œâ”€â”€ inventory-part-000000.csv
    â”œâ”€â”€ inventory-part-000001.csv
    â””â”€â”€ ...
```

ğŸ“¸ **Screenshot to add here**

* AWS Batch job â†’ SUCCEEDED
* inventory-chunks folder in S3
<img width="1919" height="775" alt="image" src="https://github.com/user-attachments/assets/deebfc3b-bf4b-4fb6-89e3-c6efa85fb7d0" />

---

## âœ… STEP 4: Chunk Processor (AWS Batch â€“ Array Job)

### ğŸ”¹ What we do

* Launch an **AWS Batch array job**
* Each container processes **one chunk**

### ğŸ”¹ Why this step is needed

* Enables **parallel processing**
* Scales automatically
* Faster processing for large inventories

### ğŸ”¹ Code used

* `docker/chunk_processor.py`

### ğŸ”¹ How array jobs work

* AWS sets `AWS_BATCH_JOB_ARRAY_INDEX`
* Index maps to a specific chunk file

### ğŸ”¹ What happens internally

1. Batch launches multiple containers
2. Each container:

   * Reads its chunk CSV
   * Converts rows to Parquet
   * Uploads output to S3

### ğŸ”¹ Output

```
s3://<destination-bucket>/processed/
â””â”€â”€ run_date=YYYY-MM-DD/
    â”œâ”€â”€ part-000000.parquet
    â”œâ”€â”€ part-000001.parquet
    â””â”€â”€ ...
```

ğŸ“¸ **Screenshot to add here**

* Batch array job showing multiple child jobs
* processed/ folder with parquet files
<img width="1919" height="835" alt="image" src="https://github.com/user-attachments/assets/95b87ab7-5a5e-4f83-9390-ec9bdad9fe81" />

---

## âœ… STEP 5: Docker Image

### ğŸ”¹ What we do

* Build **one Docker image**
* Contains both Python scripts

### ğŸ”¹ Why this design

* Reusable image
* Different behavior via command override
* Simplifies deployment

### ğŸ”¹ Files involved

* `docker/Dockerfile`
* `inventory_splitter.py`
* `chunk_processor.py`

ğŸ“¸ **Screenshot to add here**

* Docker build logs (optional)
<img width="1588" height="120" alt="image" src="https://github.com/user-attachments/assets/ed3b718d-92b5-4d4d-b96c-5d1c52ffe64a" />

---

## âœ… STEP 6: Amazon ECR

### ğŸ”¹ What we do

* Create ECR repository
* Push Docker image

### ğŸ”¹ Terraform file

* `ecr_step3.tf`

### ğŸ”¹ What happens internally

* Image is stored securely
* Batch pulls image at runtime

ğŸ“¸ **Screenshot to add here**

* ECR repository with image tag
<img width="1913" height="429" alt="image" src="https://github.com/user-attachments/assets/a0ff3caa-7c71-484d-b68d-5d491fd31353" />

---

## âœ… STEP 7: AWS Batch Core Infrastructure

### ğŸ”¹ Components created

* Compute Environment (Fargate)
* Job Queue
* Job Definitions
* IAM Roles

### ğŸ”¹ Why AWS Batch

* Handles retries
* Scales automatically
* Suitable for large batch workloads

ğŸ“¸ **Screenshot to add here**

* Batch compute environment
  <img width="1916" height="827" alt="image" src="https://github.com/user-attachments/assets/be0194d1-1dc5-4041-b1b1-3701fb2f56db" />

* Job queue
  <img width="1902" height="765" alt="image" src="https://github.com/user-attachments/assets/af711564-5b76-47b4-bf31-d74a93303b69" />

* Job definitions
<img width="1919" height="638" alt="image" src="https://github.com/user-attachments/assets/ef5a5f33-1174-4ee6-b183-aef736466c53" />
<img width="1900" height="688" alt="image" src="https://github.com/user-attachments/assets/71c4ee7e-28af-4e27-8b6b-8267779a94e8" />

---

## âœ… STEP 8: Final Parquet Output

### ğŸ”¹ What we achieve

* Analytics-ready Parquet files
* Optimized for columnar queries

### ğŸ”¹ Why Parquet

* Compression
* Faster scans
* Lower cost

ğŸ“¸ **Screenshot to add here**

* Athena / Databricks reading parquet (optional)
<img width="1919" height="835" alt="Screenshot 2026-01-03 232447" src="https://github.com/user-attachments/assets/7282cdb9-2dc7-46a5-a32a-15d6e9499a5a" />

---

## ğŸ” IAM & Security Model

* **Execution Role**

  * Pull image
  * Write logs

* **Job Role**

  * Read/write S3

* **Batch Service Role**

  * Managed by AWS

ğŸ“Œ **Compute environment has no direct permissions**

---

## ğŸš€ How to Deploy

```bash
terraform init
terraform validate
terraform plan
terraform apply
```

---

## ğŸ§  Key Learnings

* Terraform best practices
* AWS Batch with Fargate
* S3 Inventory processing
* Parallel data pipelines
* Secure IAM design

---

## ğŸ“Œ Future Enhancements

* Cross-account S3 access via VPC endpoint
* Glue Data Catalog
* CloudWatch alarms
* Databricks Autoloader

---

